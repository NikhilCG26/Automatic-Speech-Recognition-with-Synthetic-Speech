{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e82068db00434aa0bbf0b2470df535ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67b75ed3927349be8ceae92999b01f32",
              "IPY_MODEL_2b4a3093967d44f29c487ec66339987c"
            ],
            "layout": "IPY_MODEL_e04a51fb680243ecaab47a06b120c741"
          }
        },
        "67b75ed3927349be8ceae92999b01f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf4dff3ec9c4a178574074d031daa11",
            "placeholder": "​",
            "style": "IPY_MODEL_a346c087c77f4895a13ace6681157973",
            "value": "0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "2b4a3093967d44f29c487ec66339987c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_324f3f52b8fc41a8836a228fc3f2320d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2ff13f549344ec49be708a7596856b1",
            "value": 1
          }
        },
        "e04a51fb680243ecaab47a06b120c741": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf4dff3ec9c4a178574074d031daa11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a346c087c77f4895a13ace6681157973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "324f3f52b8fc41a8836a228fc3f2320d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ff13f549344ec49be708a7596856b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8UK7J-dp5iN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp \n",
        "!pip install wandb --quiet\n",
        "!pip install torchsummaryX -q"
      ],
      "metadata": {
        "id": "nYgaLmgy5iqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "k-xuWvukyH9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "yEkA_GGG-tTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Modules you require for this HW here\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummaryX import summary\n",
        "import torchnlp\n",
        "import torchnlp.nn\n",
        "from torchnlp.nn import LockedDropout\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "# import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)\n",
        "import json\n",
        "import Levenshtein\n",
        "import seaborn as sns\n",
        "import torchaudio\n",
        "import torchmetrics"
      ],
      "metadata": {
        "id": "p0aXmrxM-usO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the dataset\n",
        "!kaggle datasets download -d varunjain3/11-785-s23-hw4p2-dataset"
      ],
      "metadata": {
        "id": "F581gjfnqE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To unzip data quickly and quietly\n",
        "!unzip -q 11-785-s23-hw4p2-dataset.zip -d ./data"
      ],
      "metadata": {
        "id": "7ko7QN16qF2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBMLGYX-kZcd"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "  'batch_size': 150,\n",
        "  'lr':1e-3,\n",
        "  'epochs': 1,\n",
        "}\n",
        "\n",
        "VOCAB = ['<pad>', '<sos>', '<eos>', \n",
        "         'A',   'B',    'C',    'D',    \n",
        "         'E',   'F',    'G',    'H',    \n",
        "         'I',   'J',    'K',    'L',       \n",
        "         'M',   'N',    'O',    'P',    \n",
        "         'Q',   'R',    'S',    'T', \n",
        "         'U',   'V',    'W',    'X', \n",
        "         'Y',   'Z',    \"'\",    ' ', \n",
        "         ]\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "\n",
        "print(f\"Length of vocab: {len(VOCAB)}\")\n",
        "print(f\"Vocab: {VOCAB}\")\n",
        "print(f\"PAD_TOKEN: {PAD_TOKEN}\")\n",
        "print(f\"SOS_TOKEN: {SOS_TOKEN}\")\n",
        "print(f\"EOS_TOKEN: {EOS_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Feel free to add arguments, additional functions, this is the \n",
        "  bare-minimum template.\n",
        "  '''\n",
        "  def __init__(self, root = \"/content/data\", vocab_map = VOCAB_MAP, partition = \"train-clean-100\"):\n",
        "    self.vocab_map = vocab_map\n",
        "    self.mfcc_dir = os.path.join(root,partition)+\"/mfcc\" #TODO\n",
        "    self.transcript_dir = os.path.join(root,partition)+\"/transcripts\" #TODO\n",
        "    mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "    transcript_names    = sorted(os.listdir(self.transcript_dir))\n",
        "    assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "    self.mfccs = []#TODO\n",
        "    self.transcripts = []#TODO\n",
        "    for i in range(len(mfcc_names)):\n",
        "        mfcc        = np.load(os.path.join(self.mfcc_dir,mfcc_names[i]))\n",
        "        mfcc        = mfcc - (np.sum(mfcc, axis = 0)/mfcc.shape[0])\n",
        "        transcript  = np.load(os.path.join(self.transcript_dir,transcript_names[i])) # Remove [SOS] and [EOS] from the transcript\n",
        "        # transcript = transcript.astype(np.uint8) \n",
        "        self.mfccs.append(mfcc)\n",
        "        self.transcripts.append(transcript)\n",
        "\n",
        "    self.length = len(self.mfccs)\n",
        "    self.transcripts = [np.array([self.vocab_map[i] for i in seq]) for seq in self.transcripts]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "  \n",
        "  def __getitem__(self, ind):\n",
        "\n",
        "    mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "    transcript = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "    return mfcc, transcript\n",
        "\n",
        "  \n",
        "  def collate_fn(self, batch):\n",
        "\n",
        "    x_batch, y_batch = list(zip(*batch))\n",
        "    # batch_mfcc = []\n",
        "    # batch_transcript = []\n",
        "    # lengths_mfcc = []\n",
        "    # lengths_transcript = []\n",
        "    # for data, label in batch:\n",
        "    #   batch_mfcc.append(data)\n",
        "    #   batch_transcript.append(label)\n",
        "    #   lengths_mfcc.append(len(data))\n",
        "    #   lengths_transcript.append(len(label))\n",
        "\n",
        "    \n",
        "    x_lens      = [x.shape[0] for x in x_batch] \n",
        "    y_lens      = [y.shape[0] for y in y_batch] \n",
        "    \n",
        "    x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= PAD_TOKEN)\n",
        "    y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= PAD_TOKEN)\n",
        "\n",
        "    return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n"
      ],
      "metadata": {
        "id": "VuneWaTStdF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechDatasetTest(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Feel free to add arguments, additional functions, this is the \n",
        "  bare-minimum template.\n",
        "  '''\n",
        "  def __init__(self, root = \"/content/data\", vocab_map = VOCAB_MAP, partition = \"test-clean\"):\n",
        "    self.vocab_map = vocab_map\n",
        "    self.mfcc_dir = os.path.join(root,partition)+\"/mfcc\" #TODO\n",
        "    mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "\n",
        "\n",
        "    self.mfccs = []#TODO\n",
        "    for i in range(len(mfcc_names)):\n",
        "        mfcc        = np.load(os.path.join(self.mfcc_dir,mfcc_names[i]))\n",
        "        mfcc        = mfcc - (np.sum(mfcc, axis = 0)/mfcc.shape[0])\n",
        "        self.mfccs.append(mfcc)\n",
        "\n",
        "    self.length = len(self.mfccs)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "  \n",
        "  def __getitem__(self, ind):\n",
        "\n",
        "    mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "\n",
        "    return mfcc\n",
        "\n",
        "  \n",
        "  def collate_fn(self, batch):\n",
        "    \n",
        "    batch_mfcc = []  # TODO\n",
        "    lengths_mfcc = []\n",
        "\n",
        "    for data in batch:\n",
        "        batch_mfcc.append(data)\n",
        "        lengths_mfcc.append(len(data)) \n",
        "\n",
        "    batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True, padding_value = PAD_TOKEN)\n",
        "\n",
        "    return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ],
      "metadata": {
        "id": "L0CCK_XinTyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = SpeechDataset(partition = \"train-clean-100\") #TODO\n",
        "val_data = SpeechDataset(partition = \"dev-clean\")\n",
        "test_data = SpeechDatasetTest()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 6,\n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = train_data.collate_fn\n",
        ")#TODO\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = val_data.collate_fn\n",
        ")#TODO\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 1,\n",
        "    batch_size  = config['batch_size'], \n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn  = test_data.collate_fn\n",
        ") \n"
      ],
      "metadata": {
        "id": "tzuIXCyAuNvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader:\n",
        "    x, y, lx, ly = batch\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDAa9MsHp4a-",
        "outputId": "04348f99-8eb0-4047-add5-605a7c361ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([150, 1690, 27]) torch.Size([150, 336]) torch.Size([150]) torch.Size([150])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if you are loading the data correctly with the following:\n",
        "\n",
        "(Note: These are outputs from loading your data in the dataset class, not your dataloader which will have padded sequences)\n",
        "\n",
        "- Train Dataset\n",
        "```\n",
        "Partition loaded:  train-clean-100\n",
        "Max mfcc length:  2448\n",
        "Average mfcc length:  1264.6258453344547\n",
        "Max transcript:  400\n",
        "Average transcript length:  186.65321139493324\n",
        "```\n",
        "\n",
        "- Dev Dataset\n",
        "```\n",
        "Partition loaded:  dev-clean\n",
        "Max mfcc length:  3260\n",
        "Average mfcc length:  713.3570107288198\n",
        "Max transcript:  518\n",
        "Average transcript length:  108.71698113207547\n",
        "```\n",
        "\n",
        "- Test Dataset\n",
        "```\n",
        "Partition loaded:  test-clean\n",
        "Max mfcc length:  3491\n",
        "Average mfcc length:  738.2206106870229\n",
        "```\n",
        "\n",
        "If your values is not matching, read hints, think what could have gone wrong. Then approach TAs."
      ],
      "metadata": {
        "id": "i_n3pqt7ud4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE MODEL \n",
        "\n",
        "### Listen, Attend and Spell\n",
        "Listen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n",
        "\n",
        "- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n",
        "- LAS is known for its high accuracy and ability to improve over time with additional training data.\n",
        "- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n",
        "\n",
        "#### The Dataflow:\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "#### The Listener: \n",
        "- converts the input speech signal into a sequence of hidden states.\n",
        "\n",
        "#### The Attender:\n",
        "- Decides how the sequence of Encoder hidden state is propogated to decoder.\n",
        "\n",
        "#### The Speller:\n",
        "- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8q9wt4TwzPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Listener:"
      ],
      "metadata": {
        "id": "aPL08W_Z3R18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Psuedocode:\n",
        "```python\n",
        "class Listner:\n",
        "  def init():\n",
        "    feature_embedder = #Few layers of 1DConv-batchnorm-activation (Don't overdo)\n",
        "    pblstm_encoder = #Cascaded pblstm layers (Take pblstm from #HW3P2), \n",
        "    #can add more sequential lstms \n",
        "    dropout = #As per your liking\n",
        "\n",
        "  def forward(x,lx):\n",
        "    embedding = feature_embedder(x) #optional\n",
        "    encoding, encoding_len = pblstm_encoder(embedding/x,lx)\n",
        "    #Regularization if needed\n",
        "    return encoding, encoding_len\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_ewKlVQF3edC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "        \n",
        "class pBLSTM(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 1, batch_first = True, dropout = 0.18, bidirectional = True) # TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "    \n",
        "    \n",
        "    def forward(self, x_packed):\n",
        "        x, seq_length = pad_packed_sequence(x_packed, batch_first = True, padding_value = PAD_TOKEN)\n",
        "        x_downsampled, x_lens = self.trunc_reshape(x, seq_length)\n",
        "        # print(\"x downsampled shape: \",x_downsampled.shape)\n",
        "        x_down_packed = pack_padded_sequence(x_downsampled, x_lens, batch_first = True, enforce_sorted = False)\n",
        "        output, _ = self.blstm(x_down_packed)\n",
        "        return output, x_lens\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        batch_size, t_step, feature_dim = x.shape\n",
        "\n",
        "        if t_step % 2 != 0:\n",
        "          x = x[:,:-1,:]\n",
        "          t_step -= 1\n",
        "\n",
        "        x = x.reshape((batch_size, int(t_step/2), feature_dim*2))\n",
        "        x_lens = x_lens//2\n",
        "        return x, x_lens"
      ],
      "metadata": {
        "id": "DUj-vDXxe8ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Listener(torch.nn.Module):\n",
        "  def __init__(self, input_size, encoder_hidden_size):\n",
        "    super(Listener, self).__init__()\n",
        "    \n",
        "    self.embedding = torch.nn.Sequential(\n",
        "              torch.nn.Conv1d(in_channels=input_size,out_channels=150,kernel_size=5,padding=2),\n",
        "              torch.nn.BatchNorm1d(150),\n",
        "              torch.nn.GELU(),\n",
        "              torch.nn.Conv1d(in_channels=150,out_channels= 4*encoder_hidden_size,kernel_size=5,padding=2)\n",
        "              )#TODO: You can use CNNs as Embedding layer to extract features. Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "\n",
        "    self.lock_drop = LockedDropout(p = 0.15)\n",
        "\n",
        "    self.pblstm1 = pBLSTM(input_size = 2*2*2*encoder_hidden_size, hidden_size = 2*encoder_hidden_size)\n",
        "    self.pblstm2 = pBLSTM(input_size = 2*2*2*encoder_hidden_size, hidden_size = 2*encoder_hidden_size)\n",
        "    # self.pblstm3 = pBLSTM(input_size = 2*2*2*encoder_hidden_size, hidden_size = 2*encoder_hidden_size)\n",
        "    self.permute = PermuteBlock()\n",
        "\n",
        "  def forward(self, x, x_lens):\n",
        "    x = self.permute(x)\n",
        "    x = self.embedding(x)\n",
        "    x = self.permute(x)\n",
        "\n",
        "\n",
        "    # x = self.permute(x)\n",
        "    # x = self.lock_drop1(x)\n",
        "    # x = self.permute(x)\n",
        "\n",
        "    x = pack_padded_sequence(x, lengths = x_lens, batch_first = True, enforce_sorted = False)\n",
        "\n",
        "\n",
        "    pblstm1_out, x_lens = self.pblstm1(x)\n",
        "    pblstm1_out, x_lens = pad_packed_sequence(pblstm1_out, batch_first = True, padding_value = PAD_TOKEN)\n",
        "\n",
        "    pblstm1_out = pblstm1_out.transpose(0,1)\n",
        "    pblstm1_out = self.lock_drop(pblstm1_out)\n",
        "    pblstm1_out = pblstm1_out.transpose(0,1)\n",
        "    \n",
        "    pblstm1_out = pack_padded_sequence(pblstm1_out, lengths = x_lens, batch_first = True, enforce_sorted = False)\n",
        "\n",
        "    pblstm2_out, x_lens = self.pblstm2(pblstm1_out)\n",
        "    pblstm2_out, x_lens = pad_packed_sequence(pblstm2_out, batch_first=True, padding_value = PAD_TOKEN)\n",
        "    # pblstm2_out = self.permute(pblstm2_out)\n",
        "    # pblstm2_out = self.lock_drop(pblstm2_out)\n",
        "    # pblstm2_out = self.permute(pblstm2_out)\n",
        "    # pblstm2_out = pack_padded_sequence(pblstm2_out, lengths = x_lens, batch_first = True, enforce_sorted = False)\n",
        "\n",
        "    # pblstm3_out, x_lens = self.pblstm3(pblstm2_out)\n",
        "    # pblstm3_out, x_lens = pad_packed_sequence(pblstm3_out, batch_first=True, padding_value = PAD_TOKEN)\n",
        "\n",
        "    \n",
        "\n",
        "    return pblstm2_out, x_lens\n"
      ],
      "metadata": {
        "id": "jTQHiB1jvM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query) \n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ],
      "metadata": {
        "id": "5fG9jDZBVklL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudocode:\n",
        "\n",
        "```python\n",
        "class Attention:\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    attention_weights   = softmax(raw_weights)\n",
        "    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n",
        "\n",
        "    At the end, you can pass context through a linear layer too.\n",
        "    '''\n",
        "\n",
        "    def init(listener_hidden_size,\n",
        "              speller_hidden_size,\n",
        "              projection_size):\n",
        "\n",
        "        VW = Linear(listener_hidden_size,projection_size)\n",
        "        KW = Linear(listener_hidden_size,projection_size)\n",
        "        QW = Linear(speller_hidden_size,projection_size)\n",
        "\n",
        "    def set_key_value(encoder_outputs):\n",
        "        '''\n",
        "        In this function we take the encoder embeddings and make key and values from it.\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        '''\n",
        "        key = KW(encoder_outputs)\n",
        "        value = VW(encoder_outputs)\n",
        "      \n",
        "    def compute_context(decoder_context):\n",
        "        '''\n",
        "        In this function from decoder context, we make the query, and then we\n",
        "         multiply the queries with the keys to find the attention logits, \n",
        "         finally we take a softmax to calculate attention energy which gets \n",
        "         multiplied to the generted values and then gets summed.\n",
        "\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        query.shape = (batch_size, projection_size)\n",
        "\n",
        "        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n",
        "        '''\n",
        "        query = QW(decoder_context) #(batch_size, projection_size)\n",
        "\n",
        "        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n",
        "        #What will be the shape of raw_weights?\n",
        "\n",
        "        attention_weights = #What makes raw_weights -> attention_weights\n",
        "\n",
        "        attention_context = #Multiply attention weights to values\n",
        "\n",
        "        return attention_context, attention_weights \n",
        "```"
      ],
      "metadata": {
        "id": "QyO03fO5VotY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, listener_size, speller_size, projection_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.VW = torch.nn.Linear(listener_size,projection_size)\n",
        "    self.KW = torch.nn.Linear(listener_size,projection_size)\n",
        "    self.QW = torch.nn.Linear(speller_size,projection_size)\n",
        "    self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "  def set_key_value(self, encoder_outputs):\n",
        "    # print(encoder_outputs.shape)\n",
        "    self.key = self.KW(encoder_outputs)\n",
        "    self.value = self.VW(encoder_outputs)\n",
        "\n",
        "  def compute_context(self, decoder_context):\n",
        "\n",
        "    self.query = self.QW(decoder_context)\n",
        "    self.query = torch.unsqueeze(self.query,2)\n",
        "    length_q = self.query.shape[1]\n",
        "    # print(\"key: \", self.key.shape)\n",
        "    # print(\"q: \", self.query.shape)\n",
        "    raw_weights = torch.bmm(self.key, self.query).squeeze()\n",
        "    attention_weights = self.softmax(raw_weights/np.sqrt(length_q))\n",
        "    attention_context = torch.einsum('bi,bij->bj', attention_weights,self.value)\n",
        "\n",
        "    return attention_context, attention_weights"
      ],
      "metadata": {
        "id": "771TXxn7ViOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARMq3hI8g3xS",
        "outputId": "15a421a3-9592-473a-d811-e076bdf8737a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Speller\n",
        "\n",
        "Similar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n",
        "\n",
        "\n",
        "What you have coded till now:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>\n",
        "\n",
        "For the Speller, what we have to code:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "4Sp1WywZmm1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(4,5)\n",
        "b = torch.rand(4,6)\n",
        "torch.cat((a,b),dim=1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9eIrOPC_qPS",
        "outputId": "6519a9ba-dc98-43a0-b085-61e7946d6b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "  # Refer to your HW4P1 implementation for help with setting up the language model.\n",
        "  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size, projection_size, speller_size, attender):\n",
        "    super(). __init__()\n",
        "\n",
        "    lstm_size = embedding_size+projection_size\n",
        "    self.attend = attender # Attention object in speller\n",
        "    self.max_timesteps = 550 # Max timesteps\n",
        "    self.projection_size = projection_size\n",
        "\n",
        "    self.embedding =  torch.nn.Embedding(vocab_size, embedding_size) # Embedding layer to convert token to latent space\n",
        "\n",
        "    self.lstm_cells =  torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell(lstm_size, speller_size),\n",
        "            torch.nn.LSTMCell(speller_size, speller_size),\n",
        "            torch.nn.LSTMCell(speller_size, speller_size)\n",
        "        )# Create a sequence of LSTM Cells\n",
        "    \n",
        "\n",
        "    # For CDN (Feel free to change)\n",
        "    self.output_to_char = torch.nn.Linear(speller_size+projection_size, embedding_size)# Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n",
        "    self.activation = torch.nn.GELU()# Check which activation is suggested\n",
        "    self.dropout = torch.nn.Dropout(p=0.15)\n",
        "\n",
        "    self.linear1 = torch.nn.Linear(embedding_size, embedding_size)\n",
        "    self.linear2 = torch.nn.Linear(embedding_size, embedding_size)\n",
        "    self.char_prob = torch.nn.Linear(embedding_size, vocab_size) # Linear layer to convert hidden space back to logits for token classification\n",
        "    self.char_prob.weight = self.embedding.weight# Weight tying (From embedding layer)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def lstm_step(self, embedding, hidden_states_list):\n",
        "\n",
        "    for i in range(len(self.lstm_cells)):\n",
        "        if i == 0:\n",
        "          embedding, cell_state = self.lstm_cells[i](embedding,hidden_states_list[i]) # Feed the input through each LSTM Cell\n",
        "        else:\n",
        "          embedding, cell_state = self.lstm_cells[i](embedding,hidden_states_list[i])\n",
        "\n",
        "        hidden_states_list[i] = (embedding, cell_state)\n",
        "        \n",
        "    return embedding, hidden_states_list # What information does forward() need?\n",
        "    \n",
        "  def CDN(self, input):\n",
        "    # Make the CDN here, you can add the output-to-char\n",
        "    # print(\"input\",input.shape)\n",
        "    # print(\"lstm size\", )\n",
        "    out = self.activation(self.output_to_char(self.dropout(input)))\n",
        "    out = self.activation(self.linear1(self.dropout(out)))\n",
        "    out = self.activation(self.linear2(self.dropout(out)))\n",
        "    out = self.char_prob(out)\n",
        "    return out\n",
        "  \n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.LSTMCell):\n",
        "     torch.nn.init.uniform_(module.weight_ih, -0.1, 0.1)\n",
        "     torch.nn.init.uniform_(module.weight_hh, -0.1, 0.1)\n",
        "\n",
        "  def forward (self,x, y=None, teacher_forcing_ratio=1):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    attn_context = torch.zeros(batch_size, self.projection_size).to(DEVICE)# initial context tensor for time t = 0\n",
        "    output_symbol = torch.full((batch_size,), SOS_TOKEN).to(DEVICE) # Set it to SOS for time t = 0\n",
        "    \n",
        "    raw_outputs = []  \n",
        "    attention_plot = []\n",
        "      \n",
        "    if y is None:\n",
        "      timesteps = self.max_timesteps\n",
        "      teacher_forcing_ratio = 0 #Why does it become zero?\n",
        "\n",
        "    else:\n",
        "      timesteps = y.shape[1] # How many timesteps are we predicting for?\n",
        "\n",
        "    hidden_states_list = [None] * len(self.lstm_cells)# Initialize your hidden_states list here similar to HW4P1\n",
        "\n",
        "    for t in range(timesteps):\n",
        "      p = random.uniform(0,1)# generate a probability p between 0 and 1\n",
        "\n",
        "      if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n",
        "        output_symbol = y[:,t-1] # Take from y, else draw from probability distribution\n",
        "\n",
        "      # elif p > teacher_forcing_ratio and t > 0:\n",
        "      char_embed = self.embedding(output_symbol) # Embed the character symbol\n",
        "\n",
        "      # Concatenate the character embedding and context from attention, as shown in the diagram\n",
        "      lstm_input = torch.cat((char_embed,attn_context), dim = 1)\n",
        "\n",
        "      lstm_out, hidden_states_list = self.lstm_step(lstm_input, hidden_states_list) # Feed the input through LSTM Cells and attention.\n",
        "      # What should we retrieve from forward_step to prepare for the next timestep?\n",
        "\n",
        "      attn_context, attn_weights = self.attend.compute_context(lstm_out) # Feed the resulting hidden state into attention\n",
        "\n",
        "      cdn_input = torch.cat((lstm_out,attn_context), dim = 1)# TODO: You need to concatenate the context from the attention module with the LSTM output hidden state, as shown in the diagram\n",
        "\n",
        "      raw_pred = self.CDN(cdn_input) # call CDN with cdn_input\n",
        "\n",
        "      # Generate a prediction for this timestep and collect it in output_symbols\n",
        "      # print(raw_pred.shape)\n",
        "      output_symbol = torch.argmax(raw_pred, dim = 1)# Draw correctly from raw_pred\n",
        "\n",
        "      raw_outputs.append(raw_pred) # for loss calculation\n",
        "      attention_plot.append(attn_weights) # for plotting attention plot\n",
        "\n",
        "    \n",
        "    attention_plot = torch.stack(attention_plot, dim=1)\n",
        "    raw_outputs = torch.stack(raw_outputs, dim=1)\n",
        "\n",
        "    return raw_outputs, attention_plot"
      ],
      "metadata": {
        "id": "nFkc6MbnlUPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(5,4)\n",
        "b = torch.rand(5,3)\n",
        "c = torch.cat((a,b), dim = 1)\n"
      ],
      "metadata": {
        "id": "pwK-v2IMOBCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz-KrgnmOaCM",
        "outputId": "80b8c645-d64e-43d8-a0cb-4239050e0c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAS"
      ],
      "metadata": {
        "id": "bgOQlDRI-E4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we finally build the LAS model, comibining the listener, attender and speller together, we have given a template, but you are free to read the paper and implement it yourself."
      ],
      "metadata": {
        "id": "ZuAjQFlTBVED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LAS(torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, input_size, encoder_hidden_size, listener_size, speller_size, projection_size): # add parameters\n",
        "    super().__init__()\n",
        "    self.name = \"LAS\"\n",
        "    # Pass the right parameters here\n",
        "    self.listener = Listener(input_size, encoder_hidden_size)\n",
        "    self.attend = Attention(listener_size, speller_size, projection_size)\n",
        "    self.speller = Speller(vocab_size, embedding_size, projection_size, speller_size, self.attend)\n",
        "\n",
        "    self.aug = torch.nn.Sequential(\n",
        "        PermuteBlock(),\n",
        "        torchaudio.transforms.TimeMasking(10),\n",
        "        torchaudio.transforms.FrequencyMasking(5),\n",
        "        PermuteBlock()\n",
        "    )\n",
        "  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n",
        "    # Encode speech features\n",
        "    if self.training:\n",
        "      x = self.aug(x)\n",
        "      \n",
        "    encoder_outputs, _ = self.listener(x,lx)\n",
        "\n",
        "    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n",
        "    # Set keys and values using the encoder outputs\n",
        "    # print(encoder_outputs.shape)\n",
        "    self.attend.set_key_value(encoder_outputs)\n",
        "\n",
        "    # Decode text with the speller using context from the attention\n",
        "    raw_outputs, attention_plots = self.speller(x, y,teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "    return raw_outputs, attention_plots"
      ],
      "metadata": {
        "id": "scvB2cI-OSof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Setup "
      ],
      "metadata": {
        "id": "bPZD3vqdUisj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512 \n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this \n",
        "\n",
        "# vocab_size, speller_embedding_size, input_size, listener_hidden_size, listener_size, speller_size, projection_size\n",
        "model = LAS(\n",
        "    # Initialize your model \n",
        "    # Read the paper and think about what dimensions should be used\n",
        "    # You can experiment on these as well, but they are not requried for the early submission\n",
        "    # Remember that if you are using weight tying, some sizes need to be the same\n",
        "    vocab_size=31,\n",
        "    embedding_size=350,\n",
        "    input_size=27,\n",
        "    encoder_hidden_size=240,\n",
        "    listener_size = 4*240,\n",
        "    speller_size = 560,\n",
        "    projection_size = 300\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "summary(model, \n",
        "        x.to(DEVICE), \n",
        "        lx,\n",
        "        y.to(DEVICE))"
      ],
      "metadata": {
        "id": "a9LN0l5VUk_s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f5ded5d-b3f2-473b-985b-e27a17e40144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.18 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAS(\n",
            "  (listener): Listener(\n",
            "    (embedding): Sequential(\n",
            "      (0): Conv1d(27, 150, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): GELU(approximate='none')\n",
            "      (3): Conv1d(150, 960, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    )\n",
            "    (lock_drop): LockedDropout(p=0.15)\n",
            "    (pblstm1): pBLSTM(\n",
            "      (blstm): LSTM(1920, 480, batch_first=True, dropout=0.18, bidirectional=True)\n",
            "    )\n",
            "    (pblstm2): pBLSTM(\n",
            "      (blstm): LSTM(1920, 480, batch_first=True, dropout=0.18, bidirectional=True)\n",
            "    )\n",
            "    (permute): PermuteBlock()\n",
            "  )\n",
            "  (attend): Attention(\n",
            "    (VW): Linear(in_features=960, out_features=300, bias=True)\n",
            "    (KW): Linear(in_features=960, out_features=300, bias=True)\n",
            "    (QW): Linear(in_features=560, out_features=300, bias=True)\n",
            "    (softmax): Softmax(dim=1)\n",
            "  )\n",
            "  (speller): Speller(\n",
            "    (attend): Attention(\n",
            "      (VW): Linear(in_features=960, out_features=300, bias=True)\n",
            "      (KW): Linear(in_features=960, out_features=300, bias=True)\n",
            "      (QW): Linear(in_features=560, out_features=300, bias=True)\n",
            "      (softmax): Softmax(dim=1)\n",
            "    )\n",
            "    (embedding): Embedding(31, 350)\n",
            "    (lstm_cells): Sequential(\n",
            "      (0): LSTMCell(650, 560)\n",
            "      (1): LSTMCell(560, 560)\n",
            "      (2): LSTMCell(560, 560)\n",
            "    )\n",
            "    (output_to_char): Linear(in_features=860, out_features=350, bias=True)\n",
            "    (activation): GELU(approximate='none')\n",
            "    (dropout): Dropout(p=0.15, inplace=False)\n",
            "    (linear1): Linear(in_features=350, out_features=350, bias=True)\n",
            "    (linear2): Linear(in_features=350, out_features=350, bias=True)\n",
            "    (char_prob): Linear(in_features=350, out_features=31, bias=True)\n",
            "  )\n",
            "  (aug): Sequential(\n",
            "    (0): PermuteBlock()\n",
            "    (1): TimeMasking()\n",
            "    (2): FrequencyMasking()\n",
            "    (3): PermuteBlock()\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================================================\n",
            "                                Kernel Shape     Output Shape Params Mult-Adds\n",
            "Layer                                                                         \n",
            "0_aug.PermuteBlock_0                       -  [150, 27, 1690]      -         -\n",
            "1_aug.TimeMasking_1                        -  [150, 27, 1690]      -         -\n",
            "2_aug.FrequencyMasking_2                   -  [150, 27, 1690]      -         -\n",
            "3_aug.PermuteBlock_3                       -  [150, 1690, 27]      -         -\n",
            "4_listener.PermuteBlock_permute            -  [150, 27, 1690]      -         -\n",
            "...                                      ...              ...    ...       ...\n",
            "6060_speller.GELU_activation               -       [150, 350]      -         -\n",
            "6061_speller.Dropout_dropout               -       [150, 350]      -         -\n",
            "6062_speller.Linear_linear2       [350, 350]       [150, 350]      -    122.5k\n",
            "6063_speller.GELU_activation               -       [150, 350]      -         -\n",
            "6064_speller.Linear_char_prob      [350, 31]        [150, 31]      -    10.85k\n",
            "\n",
            "[6065 rows x 4 columns]\n",
            "---------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params           28.244141M\n",
            "Trainable params       28.244141M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             4.17085785G\n",
            "=============================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Kernel Shape     Output Shape  Params  \\\n",
              "Layer                                                                   \n",
              "0_aug.PermuteBlock_0                       -  [150, 27, 1690]     NaN   \n",
              "1_aug.TimeMasking_1                        -  [150, 27, 1690]     NaN   \n",
              "2_aug.FrequencyMasking_2                   -  [150, 27, 1690]     NaN   \n",
              "3_aug.PermuteBlock_3                       -  [150, 1690, 27]     NaN   \n",
              "4_listener.PermuteBlock_permute            -  [150, 27, 1690]     NaN   \n",
              "...                                      ...              ...     ...   \n",
              "6060_speller.GELU_activation               -       [150, 350]     NaN   \n",
              "6061_speller.Dropout_dropout               -       [150, 350]     NaN   \n",
              "6062_speller.Linear_linear2       [350, 350]       [150, 350]     NaN   \n",
              "6063_speller.GELU_activation               -       [150, 350]     NaN   \n",
              "6064_speller.Linear_char_prob      [350, 31]        [150, 31]     NaN   \n",
              "\n",
              "                                 Mult-Adds  \n",
              "Layer                                       \n",
              "0_aug.PermuteBlock_0                   NaN  \n",
              "1_aug.TimeMasking_1                    NaN  \n",
              "2_aug.FrequencyMasking_2               NaN  \n",
              "3_aug.PermuteBlock_3                   NaN  \n",
              "4_listener.PermuteBlock_permute        NaN  \n",
              "...                                    ...  \n",
              "6060_speller.GELU_activation           NaN  \n",
              "6061_speller.Dropout_dropout           NaN  \n",
              "6062_speller.Linear_linear2       122500.0  \n",
              "6063_speller.GELU_activation           NaN  \n",
              "6064_speller.Linear_char_prob      10850.0  \n",
              "\n",
              "[6065 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f31e01b-882f-4f9d-9a23-2c93ec7dbb75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_aug.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 27, 1690]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_aug.TimeMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 27, 1690]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_aug.FrequencyMasking_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 27, 1690]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_aug.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 1690, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_listener.PermuteBlock_permute</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 27, 1690]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6060_speller.GELU_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 350]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6061_speller.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 350]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6062_speller.Linear_linear2</th>\n",
              "      <td>[350, 350]</td>\n",
              "      <td>[150, 350]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>122500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6063_speller.GELU_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[150, 350]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6064_speller.Linear_char_prob</th>\n",
              "      <td>[350, 31]</td>\n",
              "      <td>[150, 31]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10850.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6065 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f31e01b-882f-4f9d-9a23-2c93ec7dbb75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f31e01b-882f-4f9d-9a23-2c93ec7dbb75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f31e01b-882f-4f9d-9a23-2c93ec7dbb75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function, Optimizers, Scheduler"
      ],
      "metadata": {
        "id": "23DMfXsaU6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer   = torch.optim.AdamW(model.parameters(), lr= config['lr']) # Feel free to experiment if needed\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='mean',ignore_index=PAD_TOKEN) #check how would you fill these values : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "scheduler   = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = 70, eta_min = 0.000001)\n",
        "\n",
        "# Optional (but Recommended): Create a custom class for a Teacher Force Schedule"
      ],
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Levenshtein Distance"
      ],
      "metadata": {
        "id": "ZWQnB8lUVY4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[int(i)])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "    for batch_idx in range(batch_size): \n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above and uncomment below for toy dataset, as the toy dataset has a list of phonemes to compare\n",
        "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "    if print_example: \n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "        wer = torchmetrics.WordErrorRate()\n",
        "        temp_wer = wer(y_string, pred_string)\n",
        "        print(\"Word Error Rate: \", temp_wer)\n",
        "        # if batch_idx ==0:\n",
        "        #   total_wer = 0\n",
        "        # total_wer +=  wer(y_string, pred_string)\n",
        "\n",
        "\n",
        "    dist/=batch_size\n",
        "    return dist, temp_wer"
      ],
      "metadata": {
        "id": "rSsiCdxPVeZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Validation functions \n"
      ],
      "metadata": {
        "id": "Pu4MrSMUUIyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            raw_predictions, attention_plot = model(x, lx, y, teacher_forcing_rate)\n",
        "\n",
        "            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
        "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "            # So in total, you have batch_size*timesteps amount of characters.\n",
        "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with \n",
        "            # your implementation\n",
        "\n",
        "            raw_predictions = torch.permute(raw_predictions, (0,2,1))\n",
        "            loss        =  criterion(raw_predictions, y)# TODO: Cross Entropy Loss\n",
        "\n",
        "            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n",
        "\n",
        "            running_loss        += loss.item()\n",
        "            running_perplexity  += perplexity.item()\n",
        "        \n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ],
      "metadata": {
        "id": "sKOdI0J5Tpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "    total_wer = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        # print(\"predictions shape: \",raw_predictions.shape)\n",
        "        greedy_predictions   = torch.argmax(raw_predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "        \n",
        "        # Calculate Levenshtein Distance\n",
        "        temp_lev_dist, temp_total_wer  = calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = True) # You can use print_example = True for one specific index i in your batches if you want\n",
        "        running_lev_dist += temp_lev_dist\n",
        "        total_wer += temp_total_wer\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist, total_wer"
      ],
      "metadata": {
        "id": "YmBLhP8cWm6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "JmZhxhNseaIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Wandb\n",
        "import wandb\n",
        "wandb.login(key=\"2cdffa7876dcc502447ef1cd147b4f60b0f4e74b\")\n",
        "# Initialize your Wandb Run Here\n",
        "run = wandb.init(\n",
        "    name = \"Run1\", ## Wandb creates random run names if you skip this field\n",
        "    #reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    id = 'nfwe2ti9',### Insert specific run id here if you want to resume a previous run\n",
        "    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"project_ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")\n",
        "# Save your model architecture in a txt file, and save the file to Wandb"
      ],
      "metadata": {
        "id": "sZcCV2BIW2R6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "e82068db00434aa0bbf0b2470df535ca",
            "67b75ed3927349be8ceae92999b01f32",
            "2b4a3093967d44f29c487ec66339987c",
            "e04a51fb680243ecaab47a06b120c741",
            "fdf4dff3ec9c4a178574074d031daa11",
            "a346c087c77f4895a13ace6681157973",
            "324f3f52b8fc41a8836a228fc3f2320d",
            "a2ff13f549344ec49be708a7596856b1"
          ]
        },
        "outputId": "89ce46de-9bbd-46c8-a5bb-f2bc70a78601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:nfwe2ti9) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e82068db00434aa0bbf0b2470df535ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.001</td></tr><tr><td>tf_ratio</td><td>1</td></tr><tr><td>train_loss</td><td>0.03857</td></tr><tr><td>valid_dist</td><td>15.34912</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Run1</strong> at: <a href='https://wandb.ai/toolflownet-feeding/project_ablations/runs/nfwe2ti9' target=\"_blank\">https://wandb.ai/toolflownet-feeding/project_ablations/runs/nfwe2ti9</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230427_225800-nfwe2ti9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:nfwe2ti9). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230427_232914-nfwe2ti9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/toolflownet-feeding/project_ablations/runs/nfwe2ti9' target=\"_blank\">Run1</a></strong> to <a href='https://wandb.ai/toolflownet-feeding/project_ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/toolflownet-feeding/project_ablations' target=\"_blank\">https://wandb.ai/toolflownet-feeding/project_ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/toolflownet-feeding/project_ablations/runs/nfwe2ti9' target=\"_blank\">https://wandb.ai/toolflownet-feeding/project_ablations/runs/nfwe2ti9</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention): \n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )"
      ],
      "metadata": {
        "id": "IgJdA9jrZwid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "PP9hZNM-v_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "#total_wer = torch.tensor(0)\n",
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "    \n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "\n",
        "    # model, dataloader, criterion, optimizer, teacher_forcing_rate\n",
        "    train_loss, running_perplexity, attention_plot = train(model,train_loader, criterion, optimizer,tf_rate)\n",
        "    valid_dist, total_wer = validate(model, val_loader)\n",
        "\n",
        "    # Print your metrics\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}\\t\".format(valid_dist))\n",
        "    print(\"\\tTeacher Forcing Ratio {}\\t\".format(tf_rate))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'lr'        : curr_lr,\n",
        "        'tf_ratio'  : tf_rate\n",
        "    })\n",
        "    print(\"Average WordErrorRate\", total_wer/19 )\n",
        "    # Plot Attention for a single item in the batch\n",
        "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "    if epoch > 10:\n",
        "      scheduler.step()\n",
        "    if (valid_dist < 30 or epoch >15) and tf_rate > 0.6:\n",
        "      tf_rate*=0.985\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_path = \"Run3_model_{0}_{1}_{2}.pth\".format(config['lr'],config['batch_size'],model.name)\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, save_path)\n",
        "        wandb.save(save_path)\n",
        "        print(\"Saved best model\")\n",
        "        # Save your model checkpoint here"
      ],
      "metadata": {
        "id": "eDWGFIcjddz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ee12f44-7f8a-49dd-fa67-e5ed28053b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/wandb/sdk/wandb_run.py:2087: UserWarning: Run (nfwe2ti9) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:   5%|▌         | 1/19 [00:02<00:48,  2.67s/it, dist=14.6533]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  THEY STAND UNMOVED IN THEIR SOLITARY GREATNESS WELL ASSURED THAT THEY ARE SEEN OF ALL THE WORLD WITHOUT ANY EFFORT TO SHOW THEMSELVES OFF AND THAT NO ONE WILL ATTEMPT TO DRIVE THEM FROM THAT POSITION\n",
            "Prediction   :  THEY STAND A MOVE IN THEIR SOLITARY GRAPES WELL AS YOU ARE THE BARE SEEN OF OLD A WORLD WITHOUT ANY EFFORT TO SHOW THEMSELVES OFF AND AT NO ONE WILL ATTEMPT TO DRIVE THEM FROM THE POSITION\n",
            "Word Error Rate:  tensor(0.3158)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  11%|█         | 2/19 [00:05<00:43,  2.55s/it, dist=15.9800]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  HE SAW THAT IN THE EXCITEMENT OF RECENT EVENTS HE HAD NOT FORMULATED A PLAN UPON THAT SCORE\n",
            "Prediction   :  HE SAW THAT IN THE EXCITEMENT EFFECING THE BENCH HE HAD NOT FORMULATED A PLAN UPON THAT SCORE\n",
            "Word Error Rate:  tensor(0.1667)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  16%|█▌        | 3/19 [00:07<00:40,  2.54s/it, dist=16.4178]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  HE CHECKED THE SILLY IMPULSE\n",
            "Prediction   :  HE TACT THIS ONLY IMPULSE\n",
            "Word Error Rate:  tensor(0.6000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  21%|██        | 4/19 [00:10<00:38,  2.56s/it, dist=16.0783]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  THE FEEBLE SON OF ARCADIUS WAS ALTERNATELY SWAYED BY HIS WIFE AND SISTER BY THE EUNUCHS AND WOMEN OF THE PALACE SUPERSTITION AND AVARICE WERE THEIR RULING PASSIONS AND THE ORTHODOX CHIEFS WERE ASSIDUOUS IN THEIR ENDEAVORS TO ALARM THE FORMER AND TO GRATIFY THE LATTER\n",
            "Prediction   :  THE FEEBLE SUN OF OUR KEITIES WAS ALTONATELY SWEED BY HIS WIFE AND SISTER BY THE UNICS AND WOMEN OF THE PALACE SUPERSTITION AND AVARICE WERE THEIR ROLLING PASSIONS AND THE ORDER OF CHEETS WERE A SIDIER IS IN THEIR ENDEAVOURS TO ALARM THE FORMER AND TO GRATIFY THE LETTER\n",
            "Word Error Rate:  tensor(0.3000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  26%|██▋       | 5/19 [00:12<00:36,  2.57s/it, dist=15.9587]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  DO HIM GOOD CURSE HIM\n",
            "Prediction   :  DO HEN GOOD CARSON\n",
            "Word Error Rate:  tensor(0.7500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  32%|███▏      | 6/19 [00:15<00:33,  2.55s/it, dist=15.7800]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  MEANWHILE ALL THE REST OF THE PEOPLE IN THE CASTLE HAD BEEN WAKENED AT THE SAME MOMENT AS THE PRINCESS AND THEY WERE NOW EXTREMELY HUNGRY\n",
            "Prediction   :  MEANWHILE ALL THE REST OF THE PEOPLE AND THE CASTLE HAD BEEN AWAKENED AT THE SAME MOMENT AS THE PRINCESS AND THEY WERE NOW EXTREMELY HONGRY\n",
            "Word Error Rate:  tensor(0.1154)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  37%|███▋      | 7/19 [00:17<00:29,  2.49s/it, dist=15.3667]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  THAT'S MACKLEWAIN'S BUSINESS\n",
            "Prediction   :  THAT'S MACHACAME'S BUSINESS\n",
            "Word Error Rate:  tensor(0.3333)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  42%|████▏     | 8/19 [00:20<00:27,  2.48s/it, dist=15.1575]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  SOMETHING HAD TO BE DONE A CLIMAX WAS NEAR AND SHE WOULD NOT SIT IDLE\n",
            "Prediction   :  SOMETHING HAD TO BE DONE A CLANCE WAS NEAR AND SHE WOULD NOT SAID IDLE\n",
            "Word Error Rate:  tensor(0.1333)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  47%|████▋     | 9/19 [00:22<00:25,  2.52s/it, dist=15.5163]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  IN THE MEANWHILE THE MINDS OF MEN CUT ADRIFT FROM THEIR ANCIENT MOORINGS WANDERED WILDLY OVER PATHLESS SEAS OF SPECULATIVE DOUBT AND ESPECIALLY IN THE MORE METAPHYSICAL AND CONTEMPLATIVE EAST ATTEMPTED TO SOLVE FOR THEMSELVES THE QUESTIONS OF MAN'S RELATION TO THE UNSEEN BY THOSE THOUSAND SCHISMS HERESIES AND THEOSOPHIES IT IS A DISGRACE TO THE WORD PHILOSOPHY TO CALL THEM BY IT ON THE RECORDS OF WHICH THE STUDENT NOW GAZES BEWILDERED UNABLE ALIKE TO COUNT OR TO EXPLAIN THEIR FANTASIES\n",
            "Prediction   :  IN THE MEANWHILE THE MINDS OF MEN COUNTED DRIFT FROM THEIR ANCIENT MOORINGS WATER WILDLY OVER PATFUL A SEAS OF SPECULES OF DOUBT AND DESPECIALLY IN THE MORE METIPHYSICAL AND CONTEMPLATIVE EAST ATTEMPTED TO SAW FOR THEMSELVES THE QUESTIONS OF MAN'S RELATION TO THE UNSEA BY THOSE THOUSAND SKISSMES HARRIS EASE AND THE ALSIFIES THE STUDENT THE WORD PHILOSOPHY TO CALL THEM BY IT ON THE RECORDS OF WHICH THE STUDENT THOU GAZED AS BE WILDERED ON THE RECORDS OF WHICH THE STUDENT THOU GAZED AS BE WILDERED ON THE RECORDS OF WHICH THE STUDENT THOU GAZED A\n",
            "Word Error Rate:  tensor(0.4796)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  53%|█████▎    | 10/19 [00:25<00:22,  2.54s/it, dist=15.8613]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  WILL HALLEY IS A BRUTE BUT I AM KEEPING MY EYES OPEN AND IF THE COAST LOOKS DANGEROUS I WILL PUT THE SHIP'S HEAD TO SEA AGAIN\n",
            "Prediction   :  WELL HOW HE IS IT BORN BUT I AM KEEP IN MY EYES OPEN AND THAT THE COAST LOOKS VANIERACE OUL PUT THE SHIP'S HEAD TO SEE AGAIN\n",
            "Word Error Rate:  tensor(0.4286)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  58%|█████▊    | 11/19 [00:27<00:20,  2.52s/it, dist=15.8855]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  THEN HE SAT DOWN IN HIS CHAIR AND GAZED WITHOUT SEEING CONTEMPLATING THE RESULT OF HIS WORK\n",
            "Prediction   :  THEN HE SAT DOWN IN HIS CHAIR AND GAZED WITHOUT SEEING CONTAIN BUTTING THE RESULT OF HIS WORK\n",
            "Word Error Rate:  tensor(0.1111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  63%|██████▎   | 12/19 [00:30<00:17,  2.52s/it, dist=16.0372]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  I ALSO OFFERED TO HELP YOUR BROTHER TO ESCAPE BUT HE WOULD NOT GO\n",
            "Prediction   :  I ALSO OFFER DANTEL DE BREAD AT THIRISK PEN BUT HE WOULD NOT GO\n",
            "Word Error Rate:  tensor(0.5000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  68%|██████▊   | 13/19 [00:32<00:14,  2.48s/it, dist=15.9785]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  WHAT THEN MY LORD\n",
            "Prediction   :  WHAT THEN MY LORD\n",
            "Word Error Rate:  tensor(0.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  74%|███████▎  | 14/19 [00:35<00:12,  2.48s/it, dist=16.0381]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  YOU'VE GOT A BIRTHDAY PRESENT THIS TIME JIM AND NO MISTAKE\n",
            "Prediction   :  YOU'VE GOT A BIRD THEY PRESENT THIS TIME JIM AND NO MISTAKE\n",
            "Word Error Rate:  tensor(0.1667)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  79%|███████▉  | 15/19 [00:37<00:09,  2.47s/it, dist=15.9271]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  GO ON DOWN THE MOUNTAIN SAID MERCURY AND AS YOU GO CAST THE BONES OF YOUR MOTHER OVER YOUR SHOULDERS BEHIND YOU AND WITH THESE WORDS HE LEAPED INTO THE AIR AND WAS SEEN NO MORE\n",
            "Prediction   :  GO ON DOWN THE MOUNTAIN SAID MARCURY AND I'ZEW GO CAST THE BOAS OF YOUR MOTHER OVER YOUR SHOULDERS BEHIND YOU AND WITH THESE WORDS HE LEAPED INTO THE AIR AND WAS SEEN NO MORE\n",
            "Word Error Rate:  tensor(0.1143)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  84%|████████▍ | 16/19 [00:40<00:07,  2.48s/it, dist=15.9742]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  DID YOU SUPPOSE A PALACE WOULD BE LIKE ONE OF OUR HANDSOME RESIDENCES ASKED THE WOMAN EVIDENTLY SURPRISED\n",
            "Prediction   :  DID YOU SUPPOSE IT HOUSE WOULD BE LIKE ONE OF OUR HANDSOME RESIDENCES ASKED THE WOMAN EVIDENTLY SURPRISED\n",
            "Word Error Rate:  tensor(0.1111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  89%|████████▉ | 17/19 [00:42<00:04,  2.50s/it, dist=15.8580]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  I FELT QUITE LIVELY MYSELF AS I MINGLED WITH THE CHRISTMAS CROWD LOOKING FOR THINGS WHICH MIGHT NOT TURN OUT TO BE ABSOLUTELY PREPOSTEROUS\n",
            "Prediction   :  I FELT QUITE LIVELY MYSELF AS A MINGLED WITH THE CHRISTMAS CROWD LOOKING FOR THINGS WHICH MIGHT NOT TURN OUT TO BE ABSOLUTELY PROPOSTEROUS\n",
            "Word Error Rate:  tensor(0.0833)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Val:  95%|█████████▍| 18/19 [00:45<00:02,  2.49s/it, dist=16.1485]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  DUE TO THEE THEIR PRAISE OF MAIDEN PURE OF TEEMING MOTHERHOOD\n",
            "Prediction   :  DUE TO BE THEIR PRAISE OF MADE ME PURE OF TEAMING MOTHERHOOD\n",
            "Word Error Rate:  tensor(0.3333)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ground Truth :  SO THEY JUST CAME IN HERE AND LIT THE CHARCOAL AND SAT DRINKING TOGETHER TILL THEY ALL FELL ASLEEP\n",
            "Prediction   :  SO THEY JUST CAME IN HERE AND LET THE CHARCOAL AND SAT DRINKING TOGETHER TILL THEY ALL FELL ASLEEP\n",
            "Word Error Rate:  tensor(0.0526)\n",
            "\tTrain Loss 0.0475\t Learning Rate 0.0010000\n",
            "\tVal Dist 15.9828\t\n",
            "\tTeacher Forcing Ratio 1\t\n",
            "Average WordErrorRate tensor(0.2682)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved best model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8D4Cu_qe9zj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "hgFYFaBGeBqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d9XK30mbCW81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SMGa-Ojv8w6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(model, dataloader):\n",
        "\n",
        "    results = []\n",
        "    model.eval()\n",
        "\n",
        "    # batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Testing\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x,lx) in enumerate(dataloader):\n",
        "\n",
        "        x, lx = x.to(DEVICE), lx\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        greedy_predictions   = torch.argmax(raw_predictions, dim=2) # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        del x, lx\n",
        "        results.extend(greedy_predictions)\n",
        "    return results"
      ],
      "metadata": {
        "id": "BunLsKdpqiCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/rerun_run2_model_0.001_150_LAS.pth\")\n",
        "model_dict = checkpoint[\"model_state_dict\"]\n",
        "# opt_dict = checkpoint[\"optimizer_state_dict\"]\n",
        "# sched_dict = checkpoint[\"scheduler_state_dict\"]\n",
        "model.load_state_dict(model_dict)\n",
        "# optimizer.load_state_dict(opt_dict)\n",
        "# scheduler.load_state_dict(sched_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-KTjIjGD6y2",
        "outputId": "ba104007-fd8b-418d-b9d2-17806f413fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = testing(model, test_loader)\n",
        "results"
      ],
      "metadata": {
        "id": "ItOfMMibGbRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dist = validate(model, val_loader)"
      ],
      "metadata": {
        "id": "_pNwEQFrijPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVkVQdKii8Kj",
        "outputId": "6b4cea2f-4568-4c4d-cb76-633af6eb6e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.850175438596492"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Load your best model Checkpoint here\n",
        "\n",
        "# TODO: Create a testing function similar to validation \n",
        "results = testing(model, test_loader)\n",
        "# TODO: Create a file with all predictions \n",
        "with open('submission_new.csv', 'w') as file:\n",
        "  file.write(\"index,label\\n\")\n",
        "  for i, pred in enumerate(results):\n",
        "    pred_sliced = indices_to_chars(results[i], VOCAB)\n",
        "    pred_string = ''.join(pred_sliced)\n",
        "    file.write(f\"{i},{pred_string}\\n\")\n"
      ],
      "metadata": {
        "id": "VxEFx7ipeCqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c 11-785-s23-hw4p2 -f submission_new.csv -m \"Finally done\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c97WQw1QE_f_",
        "outputId": "759527ea-7c66-456b-a801-b4435dddce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
            "100% 294k/294k [00:00<00:00, 1.14MB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ]
    }
  ]
}