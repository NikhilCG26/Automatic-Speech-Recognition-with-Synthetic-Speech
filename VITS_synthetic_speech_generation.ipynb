{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOrAOfx5MGvu",
        "outputId": "b3feb943-b484-41c9-ea7e-019b546458cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vits' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jaywalnut310/vits.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqlBJ-MmMMrY"
      },
      "outputs": [],
      "source": [
        "!cd /content/vits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP9qDQOxMT-Y"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/vits/requirements.txt\n",
        "!pip install Unidecode==1.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8XtF50_MbDE"
      },
      "outputs": [],
      "source": [
        "%cd /content/vits  # If you restart runtime\n",
        "%cd /content/vits/monotonic_align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np-GU1wONJrJ"
      },
      "outputs": [],
      "source": [
        "!python setup.py build_ext --inplace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcCZnKwnPVS9"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install espeak -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSOFd62lNRRN"
      },
      "outputs": [],
      "source": [
        "!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQlGYs9bN1Vu"
      },
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqIi6zdQN3uc"
      },
      "outputs": [],
      "source": [
        "%cd /content/vits\n",
        "!gdown 'https://drive.google.com/uc?id=1q86w74Ygw2hNzYP9cWkeClGT5X25PvBT'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/vits"
      ],
      "metadata": {
        "id": "dI_QqPv2LmGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/vits/monotonic_align\n",
        "# !python /content/vits/monotonic_align/setup.py build_ext --inplace"
      ],
      "metadata": {
        "id": "E03ZyPQ7AqQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phonemizer"
      ],
      "metadata": {
        "id": "SlgsDfmnCTpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2dwLM5uNVWm"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import commons\n",
        "import utils\n",
        "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
        "from models import SynthesizerTrn\n",
        "from text.symbols import symbols\n",
        "from text import text_to_sequence\n",
        "\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "\n",
        "def get_text(text, hps):\n",
        "    text_norm = text_to_sequence(text, hps.data.text_cleaners)\n",
        "    if hps.data.add_blank:\n",
        "        text_norm = commons.intersperse(text_norm, 0)\n",
        "    text_norm = torch.LongTensor(text_norm).to(device)\n",
        "    return text_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_0ugxNdNYzb"
      },
      "outputs": [],
      "source": [
        "hps = utils.get_hparams_from_file(\"./configs/ljs_base.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/vits"
      ],
      "metadata": {
        "id": "qENTTkR0j1Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysasJm_XSAFb"
      },
      "source": [
        "## Multi Speaker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMQ40ddISOTa"
      },
      "outputs": [],
      "source": [
        "!gdown 'https://drive.google.com/uc?id=11aHOlhnxzjpdWDpsz1vFDCzbeEfoIxru'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kySs9k1wREG_"
      },
      "outputs": [],
      "source": [
        "hps_ms = utils.get_hparams_from_file(\"./configs/vctk_base.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kSLnvBKRJFg"
      },
      "outputs": [],
      "source": [
        "net_g_ms = SynthesizerTrn(\n",
        "    len(symbols),\n",
        "    hps_ms.data.filter_length // 2 + 1,\n",
        "    hps_ms.train.segment_size // hps.data.hop_length,\n",
        "    n_speakers=hps_ms.data.n_speakers,\n",
        "    **hps_ms.model)\n",
        "# _ = net_g.eval()\n",
        "\n",
        "_ = utils.load_checkpoint(\"pretrained_vctk.pth\", net_g_ms, None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from vits.text import text_to_sequence"
      ],
      "metadata": {
        "id": "4CCpvW3Ccb1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import os\n",
        "  import torchaudio\n",
        "  from tqdm import tqdm\n",
        "\n",
        "# Set the device to use for inference\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define a function to handle the error and skip files\n",
        "    # def handle_error(file_path):\n",
        "    #   print(f\"{file_path} has been skipped.\")\n",
        "\n",
        "# Loop through all the text files in the directory\n",
        "# for file_name in os.listdir('/content'):\n",
        "#   if file_name.endswith('.txt'):\n",
        "#     # Open the file\n",
        "  with open(\"/content/concatenated_sentences.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "  id = 0 # Initialize speaker identity\n",
        "  i = 1;\n",
        "    \n",
        "  for line in tqdm(lines):\n",
        "    if i >= 53762: ###### Input the file numbers to iterate through\n",
        "      if i < 55000:\n",
        "        try:\n",
        "            line = line.strip()\n",
        "            \n",
        "            # Extract the number and sentence from the line\n",
        "            num, sentence = line.split(\" \", 1)\n",
        "            stn_tst = get_text(sentence, hps_ms)\n",
        "            # print(sentence)\n",
        "            # Check if the speaker identity needs to be updated\n",
        "            if i % 20 == 1:\n",
        "                id += 1\n",
        "                id = id % 108 # There are 82 speakers\n",
        "            \n",
        "            # Generate the audio for the sentence\n",
        "            with torch.no_grad():\n",
        "                # Prepare the input tensors\n",
        "                x_tst = stn_tst.unsqueeze(0)\n",
        "                x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
        "                sid = torch.LongTensor([id]).to(device)\n",
        "\n",
        "                # Move the tensors to the device\n",
        "                net_g_ms = net_g_ms.to(device)\n",
        "                x_tst = x_tst.to(device)\n",
        "                # print(x_tst)\n",
        "                x_tst_lengths = x_tst_lengths.to(device)\n",
        "                # print(x_tst_lengths)\n",
        "                audio = net_g_ms.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0, 0].data.float().cpu().numpy()\n",
        "\n",
        "                file_name = f\"{num}.wav\"\n",
        "                num_channels = 1  # mono audio\n",
        "                num_samples = len(audio)\n",
        "                audio_tensor = torch.from_numpy(audio).view(num_channels, num_samples)  # convert to 2D tensor\n",
        "                folder = \"/content/audio\"\n",
        "                if not os.path.exists(folder):\n",
        "                    os.makedirs(folder)\n",
        "                file_path = os.path.join(folder, file_name)\n",
        "                torchaudio.save(file_path, audio_tensor, hps_ms.data.sampling_rate)\n",
        "            \n",
        "            i += 1\n",
        "            \n",
        "        except:\n",
        "            print(f\"{num} has been skipped\")\n",
        "            i += 1\n",
        "            \n",
        "    else:\n",
        "        i += 1\n"
      ],
      "metadata": {
        "id": "aP89RtYdkzHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "vcXIB3jazCc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://sdk.cloud.google.com | bash"
      ],
      "metadata": {
        "id": "7258hfq8y2Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud init"
      ],
      "metadata": {
        "id": "_KdgKRGty3IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/audio gs://idl_project1/Train-Clean-360/"
      ],
      "metadata": {
        "id": "VIKD2d_Nzy3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/audio/1472-139797-0039.wav')"
      ],
      "metadata": {
        "id": "cRdqdZbsSDjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # !zip -r /content/audio_zip.zip /content/audio"
      ],
      "metadata": {
        "id": "posuxafilr0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://sdk.cloud.google.com | bash\n",
        "!exec -l $SHELL\n",
        "!gcloud init"
      ],
      "metadata": {
        "id": "tdCDqgkr2FgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m cp -r /content/audio gs://idl_project1/\n"
      ],
      "metadata": {
        "id": "UuAawOPd3fdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt4OrRoyRItZ"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# Set the device to use for inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "with open(\"/content/combined_text.txt\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "\n",
        "id = 0 # Initialize speaker identity\n",
        "i = 1;\n",
        "for line in tqdm(lines):\n",
        "  if i >=7167:\n",
        "    if i<15000:\n",
        "      line = line.strip()\n",
        "      \n",
        "      # Extract the number and sentence from the line\n",
        "      # num, sentence = line.split(\" \", 1)\n",
        "      # stn_tst = get_text(sentence, hps_ms)\n",
        "      # num = int(num)\n",
        "      print(num)\n",
        "      print(sentence)\n",
        "      # Check if the speaker identity needs to be updated\n",
        "      if i % 20 == 1:\n",
        "        id += 1\n",
        "        id = id % 108 # There are 82 speakers\n",
        "        i+=1;\n",
        "    # Generate the audio for the sentence\n",
        "    \n",
        "    # sid = torch.LongTensor([id]) # speaker identity\n",
        "    # sid = sid.cuda()\n",
        "\n",
        "    # stn_tst = get_text(\"Can I test this model with different people please\", hps_ms)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        # Prepare the input tensors\n",
        "        x_tst = stn_tst.unsqueeze(0)\n",
        "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
        "        sid = torch.LongTensor([id]).to(device)\n",
        "\n",
        "        # Move the tensors to the device\n",
        "        net_g_ms = net_g_ms.to(device)\n",
        "        x_tst = x_tst.to(device)\n",
        "        x_tst_lengths = x_tst_lengths.to(device)\n",
        "        audio = net_g_ms.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0, 0].data.float().cpu().numpy()\n",
        "\n",
        "        file_name = f\"{num}.wav\"\n",
        "        num_channels = 1  # mono audio\n",
        "        num_samples = len(audio)\n",
        "        audio_tensor = torch.from_numpy(audio).view(num_channels, num_samples)  # convert to 2D tensor\n",
        "        folder = \"/content/audio\"\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "        file_path = os.path.join(folder, file_name)\n",
        "        torchaudio.save(file_path, audio_tensor, hps_ms.data.sampling_rate)\n",
        "    else:\n",
        "      i+=1;\n",
        "  else:\n",
        "    i +=1;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "id": "4EcqwrKcDreH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project "
      ],
      "metadata": {
        "id": "XWqxIs5CD3Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project verdant-wave-384615"
      ],
      "metadata": {
        "id": "ujZAG-9jEJ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--4_eNrISE_6"
      },
      "source": [
        "## Voice Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyhc7sviTuAG"
      },
      "outputs": [],
      "source": [
        "!wget https://jaywalnut310.github.io/vits-demo/wavs/vc_01_01.wav"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mVGMy7CZHVP"
      },
      "outputs": [],
      "source": [
        "from mel_processing import spectrogram_torch\n",
        "from utils import load_wav_to_torch\n",
        "\n",
        "audio, sampling_rate = load_wav_to_torch(\"./vc_01_01.wav\")\n",
        "        \n",
        "y = audio / hps_ms.data.max_wav_value\n",
        "y = y.unsqueeze(0)\n",
        "\n",
        "spec = spectrogram_torch(y, hps_ms.data.filter_length,\n",
        "    hps_ms.data.sampling_rate, hps_ms.data.hop_length, hps_ms.data.win_length,\n",
        "    center=False)\n",
        "spec_lengths = torch.LongTensor([spec.size(-1)])\n",
        "sid_src = torch.LongTensor([81])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26cbEY7aRIfA"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    sid_tgt1 = torch.LongTensor([77])\n",
        "    sid_tgt2 = torch.LongTensor([14])\n",
        "    sid_tgt3 = torch.LongTensor([1])\n",
        "    sid_tgt4 = torch.LongTensor([17])\n",
        "    audio1 = net_g_ms.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt1)[0][0,0].data.float().numpy()\n",
        "    audio2 = net_g_ms.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt2)[0][0,0].data.float().numpy()\n",
        "    audio3 = net_g_ms.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt3)[0][0,0].data.float().numpy()\n",
        "    audio4 = net_g_ms.voice_conversion(spec, spec_lengths, sid_src=sid_src, sid_tgt=sid_tgt4)[0][0,0].data.float().numpy()\n",
        "print(\"Original SID: %d\" % sid_src.item())\n",
        "ipd.display(ipd.Audio(y[0].numpy(), rate=hps_ms.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt1.item())\n",
        "ipd.display(ipd.Audio(audio1, rate=hps_ms.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt2.item())\n",
        "ipd.display(ipd.Audio(audio2, rate=hps_ms.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt3.item())\n",
        "ipd.display(ipd.Audio(audio3, rate=hps_ms.data.sampling_rate))\n",
        "print(\"Converted SID: %d\" % sid_tgt4.item())\n",
        "ipd.display(ipd.Audio(audio4, rate=hps_ms.data.sampling_rate))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}